--------------------------------------
Running PBS prologue script
--------------------------------------
User and Job Data:
--------------------------------------
Job ID:    646168.qb3
Username:  fchen14
Group:     loniadmin
Date:      07-May-2019 20:54
Node:      qb028 (9642)
--------------------------------------
PBS has allocated the following nodes:

qb028
qb043
qb045
qb046
qb051
qb059
qb064
qb066
qb086
qb087
qb088
qb091
qb093
qb096
qb098
qb099

A total of 320 processors on 16 nodes allocated
---------------------------------------------
Check nodes and clean them of stray processes
---------------------------------------------
Checking node qb028 20:54:43 
Checking node qb043 20:54:45 
Checking node qb045 20:54:46 
Checking node qb046 20:54:48 
Checking node qb051 20:54:50 
Checking node qb059 20:54:51 
Checking node qb064 20:54:53 
Checking node qb066 20:54:55 
Checking node qb086 20:54:57 
Checking node qb087 20:54:58 
Checking node qb088 20:55:00 
Checking node qb091 20:55:02 
Checking node qb093 20:55:03 
Checking node qb096 20:55:05 
Checking node qb098 20:55:07 
Checking node qb099 20:55:08 
Done clearing all the allocated nodes
------------------------------------------------------
Concluding PBS prologue script - 07-May-2019 20:55:08
------------------------------------------------------
using 32 processes...
--------------------------------
Proc 20: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 21: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 26: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 27: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 24: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 25: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 6: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 7: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 2: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 3: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 16: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 17: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 18: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 19: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 8: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 9: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 28: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 29: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 11: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 10: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 23: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 22: *** testing PHDF5 dataset collective read...
--------------------------------
Input file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5
Output file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5.res_all.h5
--------------------------------
Proc 0: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 1: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 13: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 12: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 14: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 15: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 4: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 5: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 30: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 31: *** testing PHDF5 dataset collective read...
--------------------------------
Reading time is 280.996
Elapsed time is 9812.030
Writing time is 3.304
===================================
PHDF5 tests finished with no errors
===================================
omp run  trd=1, 10098 sec
--------------------------------
Proc 4: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 5: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 22: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 23: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 2: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 12: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 13: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 6: *** testing PHDF5 dataset collective read...
--------------------------------
Input file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5
Output file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5.res_all.h5
--------------------------------
Proc 0: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 1: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 3: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 17: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 16: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 8: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 18: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 7: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 28: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 14: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 9: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 19: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 11: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 29: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 20: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 15: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 30: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 21: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 24: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 25: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 10: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 31: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 26: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 27: *** testing PHDF5 dataset collective read...
--------------------------------
Reading time is 265.013
Elapsed time is 5058.227
Writing time is 2.221
===================================
PHDF5 tests finished with no errors
===================================
omp run  trd=2, 5328 sec
--------------------------------
Proc 8: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 16: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 9: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 18: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 14: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 19: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 17: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 15: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 13: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 11: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 12: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 10: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 4: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 5: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 2: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 22: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 28: *** testing PHDF5 dataset collective read...
--------------------------------
Input file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5
Output file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5.res_all.h5
--------------------------------
Proc 0: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 23: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 3: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 29: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 27: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 26: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 1: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 20: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 21: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 6: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 24: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 30: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 31: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 7: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 25: *** testing PHDF5 dataset collective read...
--------------------------------
Reading time is 263.054
Elapsed time is 2225.927
Writing time is 1.793
===================================
PHDF5 tests finished with no errors
===================================
omp run  trd=5, 2492 sec
--------------------------------
Proc 10: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 11: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 14: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 15: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 6: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 12: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 13: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 21: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 26: *** testing PHDF5 dataset collective read...
--------------------------------
Input file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5
Output file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5.res_all.h5
--------------------------------
Proc 0: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 20: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 27: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 3: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 7: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 23: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 2: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 8: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 1: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 5: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 9: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 22: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 28: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 29: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 4: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 16: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 30: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 18: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 19: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 31: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 17: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 24: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 25: *** testing PHDF5 dataset collective read...
--------------------------------
Reading time is 233.353
Elapsed time is 1208.459
Writing time is 2.945
===================================
PHDF5 tests finished with no errors
===================================
omp run  trd=10, 1447 sec
MPI process 12 is on GPU 0
--------------------------------
Proc 12: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 20 is on GPU 0
--------------------------------
Proc 20: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 26 is on GPU 0
--------------------------------
Proc 26: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 30 is on GPU 0
--------------------------------
Proc 30: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 22 is on GPU 0
--------------------------------
Proc 22: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 28 is on GPU 0
--------------------------------
Proc 28: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 8 is on GPU 0
--------------------------------
Proc 8: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 6 is on GPU 0
--------------------------------
Proc 6: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 18 is on GPU 0
--------------------------------
Proc 18: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 4 is on GPU 0
--------------------------------
Proc 4: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 0 is on GPU 0
Input file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5
Output file is: hdf5t/sample_kinase_phosphatase_receptor_protease.h5.res_all.h5
--------------------------------
Proc 0: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 13 is on GPU 1
--------------------------------
Proc 13: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 29 is on GPU 1
--------------------------------
Proc 29: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 9 is on GPU 1
--------------------------------
Proc 9: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 21 is on GPU 1
--------------------------------
Proc 21: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 24 is on GPU 0
--------------------------------
Proc 24: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 27 is on GPU 1
--------------------------------
Proc 27: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 23 is on GPU 1
--------------------------------
Proc 23: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 31 is on GPU 1
--------------------------------
Proc 31: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 7 is on GPU 1
--------------------------------
Proc 7: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 19 is on GPU 1
--------------------------------
Proc 19: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 16 is on GPU 0
--------------------------------
Proc 16: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 14 is on GPU 0
--------------------------------
Proc 14: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 5 is on GPU 1
--------------------------------
Proc 5: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 1 is on GPU 1
--------------------------------
Proc 1: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 10 is on GPU 0
--------------------------------
Proc 10: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 2 is on GPU 0
--------------------------------
Proc 2: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 25 is on GPU 1
--------------------------------
Proc 25: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 17 is on GPU 1
--------------------------------
Proc 17: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 15 is on GPU 1
--------------------------------
Proc 15: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 11 is on GPU 1
--------------------------------
Proc 11: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 3 is on GPU 1
--------------------------------
Proc 3: *** testing PHDF5 dataset collective read...
--------------------------------
Reading time is 206.567
Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2672820224
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2b9b71e12c00 device:0x1308240000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b9b71e14d90 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b9b90000010 device:0x1308340000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2675834880
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2ada41a12be0 device:0x1308240000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2ada41a14d70 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2ada60000010 device:0x1308340000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2606497792
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2b39d89e0460 device:0x1304120000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b39d89e25f0 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b39f0000010 device:0x1304220000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2675834880
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2b67fd68d220 device:0x1308240000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b67fd68f3b0 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b6810000010 device:0x1308340000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2672820224
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2b0c798b6800 device:0x1308240000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b0c798b8990 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b0c90000010 device:0x1308340000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2606497792
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2b5844dae9a0 device:0x1304120000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b5844db0b30 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b5860000010 device:0x1304220000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3222065664 thread:1
Out of memory allocating 3222065376 bytes of device memory
call to cuMemAlloc returned error 2: Out of memory

total/free CUDA memory: 5977800704/2675834880
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2b164c612bf0 device:0x1308240000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b164c614d80 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b1660000010 device:0x1308340000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2672820224
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2c4f8e0 device:0x1308240000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2c51a70 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b3f80000010 device:0x1308340000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2606497792
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2b665c5dfdc0 device:0x1304120000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b665c5e1f50 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b6670000010 device:0x1304220000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2672820224
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2bcef50 device:0x1308240000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2bd10e0 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b5390000010 device:0x1308340000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2605056000
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2b4573558cc0 device:0x1304120000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b457355ae50 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b4590000010 device:0x1304220000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2605056000
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x1fe6c50 device:0x1304120000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x1fe8de0 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b10d0000010 device:0x1304220000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2603614208
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2ba653958e30 device:0x1304120000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2ba65395afc0 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2ba670000010 device:0x1304220000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2675834880
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2a63ab0 device:0x1308240000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2a65c40 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2afe50000010 device:0x1308340000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2606497792
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2799c40 device:0x1304120000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x279bdd0 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2ba580000010 device:0x1304220000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2606497792
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2b3daa5af930 device:0x1304120000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b3daa5b1ac0 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b3dc0000010 device:0x1304220000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

total/free CUDA memory: 5977800704/2675834880
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2ae64928d3c0 device:0x1308240000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2ae64928f550 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2ae660000010 device:0x1308340000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2675834880
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x31a8ac0 device:0x1308240000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x31aac50 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2ac910000010 device:0x1308340000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2603483136
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2b9d8d7dd860 device:0x1304120000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b9d8d7df9f0 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b9da0000010 device:0x1304220000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

total/free CUDA memory: 5977800704/2672820224
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2b4424a4af10 device:0x1308240000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b4424a4d0a0 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b4440000010 device:0x1308340000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2606104576
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2b944459dfc0 device:0x1304120000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b94445a0150 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b9460000010 device:0x1304220000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2675834880
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x3797fb0 device:0x1308240000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x379a140 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b8550000010 device:0x1308340000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2675834880
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2b14724b67c0 device:0x1308240000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b14724b8950 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b1490000010 device:0x1308340000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3222065664 thread:1
total/free CUDA memory: 5977800704/2606497792
call to cuMemAlloc returned error 2: Out of memory

Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2ab9345e0020 device:0x1304120000 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2ab9345e21b0 device:0x1304122200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2ab950000010 device:0x1304220000 size:3222065376 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3222065664 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3225071034 bytes of device memory
total/free CUDA memory: 5977800704/2670723072
Present table dump for device[1]: NVIDIA Tesla GPU 0, compute capability 3.5, threadid=1
host:0x2b9b47ede580 device:0x1304120000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b9b47ee0710 device:0x1304122200 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b9b60000010 device:0x1304220000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1304120000 size:8704 thread:1
allocated block device:0x1304122200 size:8704 thread:1
allocated block device:0x1304220000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

Out of memory allocating 3222065376 bytes of device memory
total/free CUDA memory: 5977800704/2672820224
Present table dump for device[2]: NVIDIA Tesla GPU 1, compute capability 3.5, threadid=1
host:0x2b62d41d7c50 device:0x1308240000 size:8584 presentcount:1+0 line:445 name:dvc_blk_part_a
host:0x2b62d41d9de0 device:0x1308242200 size:8576 presentcount:1+0 line:445 name:dvc_blk_part_b
host:0x2b62f0000010 device:0x1308340000 size:3225071034 presentcount:1+0 line:445 name:dvc_blk_part_a
allocated block device:0x1308240000 size:8704 thread:1
allocated block device:0x1308242200 size:8704 thread:1
allocated block device:0x1308340000 size:3225071104 thread:1
call to cuMemAlloc returned error 2: Out of memory

[qb099:mpi_rank_30][handle_cqe] Send desc error in msg to 26, wc_opcode=0
[qb099:mpi_rank_30][handle_cqe] Msg from 26: wc.status=12, wc.wr_id=0x21baf30, wc.opcode=0, vbuf->phead->type=0 = MPIDI_CH3_PKT_EAGER_SEND
[qb099:mpi_rank_30][handle_cqe] src/mpid/ch3/channels/mrail/src/gen2/ibv_channel_manager.c:548: [] Got completion with error 12, vendor code=0x81, dest rank=26
: Invalid argument (22)
[qb099:mpi_rank_31][handle_cqe] Send desc error in msg to 27, wc_opcode=0
[qb099:mpi_rank_31][handle_cqe] Msg from 27: wc.status=12, wc.wr_id=0x27e3040, wc.opcode=0, vbuf->phead->type=0 = MPIDI_CH3_PKT_EAGER_SEND
[qb099:mpi_rank_31][handle_cqe] src/mpid/ch3/channels/mrail/src/gen2/ibv_channel_manager.c:548: [] Got completion with error 12, vendor code=0x81, dest rank=27
: File exists (17)
[qb098:mpi_rank_28][handle_cqe] Send desc error in msg to 24, wc_opcode=0
[qb098:mpi_rank_28][handle_cqe] Msg from 24: wc.status=12, wc.wr_id=0x1d721a0, wc.opcode=0, vbuf->phead->type=0 = MPIDI_CH3_PKT_EAGER_SEND
[qb098:mpi_rank_28][handle_cqe] src/mpid/ch3/channels/mrail/src/gen2/ibv_channel_manager.c:548: [] Got completion with error 12, vendor code=0x81, dest rank=24
: File exists (17)
[qb098:mpi_rank_29][handle_cqe] Send desc error in msg to 25, wc_opcode=0
[qb098:mpi_rank_29][handle_cqe] Msg from 25: wc.status=12, wc.wr_id=0x10732b0, wc.opcode=0, vbuf->phead->type=0 = MPIDI_CH3_PKT_EAGER_SEND
[qb098:mpi_rank_29][handle_cqe] src/mpid/ch3/channels/mrail/src/gen2/ibv_channel_manager.c:548: [] Got completion with error 12, vendor code=0x81, dest rank=25
: File exists (17)
gpu run , 759 sec
input_file =  hdf5t/sample_kinase_phosphatase_receptor_protease.h5
save csv =  True
validate =  False
keys= ['cosine', 'generalised', 'normal', 'sarika', 'start_loc', 'wu']
protein_file_name: hdf5t/sample_kinase_phosphatase_receptor_protease.json
Traceback (most recent call last):
  File "rebuild_mat.py", line 130, in <module>
    with open(pn_filename) as json_file:  
FileNotFoundError: [Errno 2] No such file or directory: 'hdf5t/sample_kinase_phosphatase_receptor_protease.json'
------------------------------------------------------
Running PBS epilogue script    - 08-May-2019 02:30:43
------------------------------------------------------
Checking node qb028 (MS)
Checking node qb099 ok
Checking node qb098 ok
Checking node qb096 ok
Checking node qb093 ok
Checking node qb091 ok
Checking node qb088 ok
Checking node qb087 ok
Checking node qb086 ok
Checking node qb066 ok
Checking node qb064 ok
Checking node qb059 ok
Checking node qb051 ok
Checking node qb046 ok
Checking node qb045 ok
Checking node qb043 ok
Checking node qb028 ok
------------------------------------------------------
Concluding PBS epilogue script - 08-May-2019 02:31:10
------------------------------------------------------
Exit Status:     1
Job ID:          646168.qb3
Username:        fchen14
Group:           loniadmin
Job Name:        tn16_p32.8k.pbs
Session Id:      9641
Resource Limits: ncpus=1,neednodes=16:ppn=20,nodes=16:ppn=20,walltime=72:00:00
Resources Used:  cput=24:26:22,mem=2694940kb,vmem=3242696kb,walltime=05:35:33
Queue Used:      checkpt
Account String:  loni_loniadmin1
Node:            qb028
Process id:      15022
Last status:     08-May-2019:02:05 PBS_job=646168.qb3 user=fchen14 allocation=loni_loniadmin1 queue=checkpt total_load=44.82 cpu_hours=0.00 wall_hours=4.26 unused_nodes=0 total_nodes=16 ppn=20 avg_load=2.80 avg_cpu=148% avg_mem=5513mb avg_vmem=12924mb top_proc=fchen14:pgi:qb066:6.2G:3.0G:0.0hr:101% toppm=fchen14:pgi.mpi.pomp.out:qb028:6285M:6200M node_processes=4 avg_avail_mem=47858mb min_avail_mem=45515mb reverified_avg_load=19.99
------------------------------------------------------
