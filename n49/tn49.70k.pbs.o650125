--------------------------------------
Running PBS prologue script
--------------------------------------
User and Job Data:
--------------------------------------
Job ID:    650125.qb3
Username:  fchen14
Group:     loniadmin
Date:      20-May-2019 20:13
Node:      qb063 (3552)
--------------------------------------
PBS has allocated the following nodes:

qb063
qb064
qb065
qb066
qb067
qb068
qb069
qb071
qb072
qb073
qb074
qb075
qb076
qb077
qb078
qb080
qb081
qb082
qb083
qb084
qb085
qb086
qb087
qb088
qb089
qb090
qb092
qb093
qb094
qb095
qb096
qb097
qb098
qb101
qb112
qb113
qb114
qb115
qb116
qb117
qb118
qb119
qb124
qb127
qb132
qb133
qb134
qb135
qb146

A total of 980 processors on 49 nodes allocated
---------------------------------------------
Check nodes and clean them of stray processes
---------------------------------------------
Checking node qb063 20:13:05 
Checking node qb064 20:13:07 
Checking node qb065 20:13:09 
Checking node qb066 20:13:10 
Checking node qb067 20:13:12 
Checking node qb068 20:13:14 
Checking node qb069 20:13:15 
Checking node qb071 20:13:17 
Checking node qb072 20:13:19 
Checking node qb073 20:13:21 
Checking node qb074 20:13:22 
Checking node qb075 20:13:24 
Checking node qb076 20:13:25 
Checking node qb077 20:13:27 
Checking node qb078 20:13:29 
Checking node qb080 20:13:31 
Checking node qb081 20:13:32 
Checking node qb082 20:13:34 
Checking node qb083 20:13:36 
Checking node qb084 20:13:37 
Checking node qb085 20:13:39 
Checking node qb086 20:13:41 
Checking node qb087 20:13:43 
Checking node qb088 20:13:44 
Checking node qb089 20:13:46 
Checking node qb090 20:13:48 
Checking node qb092 20:13:49 
Checking node qb093 20:13:51 
Checking node qb094 20:13:53 
Checking node qb095 20:13:55 
Checking node qb096 20:13:56 
Checking node qb097 20:13:58 
Checking node qb098 20:14:00 
Checking node qb101 20:14:01 
Checking node qb112 20:14:03 
Checking node qb113 20:14:05 
Checking node qb114 20:14:07 
Checking node qb115 20:14:08 
Checking node qb116 20:14:10 
Checking node qb117 20:14:12 
Checking node qb118 20:14:13 
Checking node qb119 20:14:15 
Checking node qb124 20:14:17 
Checking node qb127 20:14:19 
Checking node qb132 20:14:20 
Checking node qb133 20:14:22 
Checking node qb134 20:14:24 
Checking node qb135 20:14:25 
Checking node qb146 20:14:27 
Done clearing all the allocated nodes
------------------------------------------------------
Concluding PBS prologue script - 20-May-2019 20:14:27
------------------------------------------------------
/project/fchen14/packages/openmpi-4.0.1/install/bin/mpirun
--------------------------------------------------------------------------
By default, for Open MPI 4.0 and later, infiniband ports on a device
are not used by default.  The intent is to use UCX for these devices.
You can override this policy by setting the btl_openib_allow_ib MCA parameter
to true.

  Local host:              qb118
  Local adapter:           mlx4_0
  Local port:              1

--------------------------------------------------------------------------
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   qb118
  Local device: mlx4_0
--------------------------------------------------------------------------
MPI process 94 is on GPU 0
--------------------------------
Proc 94: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 30 is on GPU 0
--------------------------------
Proc 30: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 66 is on GPU 0
--------------------------------
Proc 66: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 32 is on GPU 0
--------------------------------
Proc 32: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 34 is on GPU 0
--------------------------------
Proc 34: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 24 is on GPU 0
--------------------------------
Proc 24: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 96 is on GPU 0
--------------------------------
Proc 96: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 62 is on GPU 0
--------------------------------
Proc 62: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 12 is on GPU 0
--------------------------------
Proc 12: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 20 is on GPU 0
--------------------------------
Proc 20: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 84 is on GPU 0
--------------------------------
Proc 84: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 26 is on GPU 0
--------------------------------
Proc 26: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 90 is on GPU 0
--------------------------------
Proc 90: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 54 is on GPU 0
--------------------------------
Proc 54: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 22 is on GPU 0
--------------------------------
Proc 22: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 92 is on GPU 0
--------------------------------
Proc 92: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 36 is on GPU 0
--------------------------------
Proc 36: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 80 is on GPU 0
--------------------------------
Proc 80: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 95 is on GPU 1
--------------------------------
Proc 95: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 31 is on GPU 1
--------------------------------
Proc 31: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 74 is on GPU 0
--------------------------------
Proc 74: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 14 is on GPU 0
--------------------------------
Proc 14: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 33 is on GPU 1
--------------------------------
Proc 33: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 8 is on GPU 0
--------------------------------
Proc 8: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 35 is on GPU 1
--------------------------------
Proc 35: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 18 is on GPU 0
--------------------------------
Proc 18: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 67 is on GPU 1
--------------------------------
Proc 67: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 44 is on GPU 0
--------------------------------
Proc 44: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 28 is on GPU 0
--------------------------------
Proc 28: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 58 is on GPU 0
--------------------------------
Proc 58: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 88 is on GPU 0
--------------------------------
Proc 88: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 50 is on GPU 0
--------------------------------
Proc 50: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 76 is on GPU 0
--------------------------------
Proc 76: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 60 is on GPU 0
--------------------------------
Proc 60: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 78 is on GPU 0
--------------------------------
Proc 78: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 25 is on GPU 1
--------------------------------
Proc 25: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 2 is on GPU 0
--------------------------------
Proc 2: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 48 is on GPU 0
--------------------------------
Proc 48: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 16 is on GPU 0
--------------------------------
Proc 16: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 72 is on GPU 0
--------------------------------
Proc 72: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 6 is on GPU 0
--------------------------------
Proc 6: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 52 is on GPU 0
--------------------------------
Proc 52: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 10 is on GPU 0
--------------------------------
Proc 10: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 97 is on GPU 1
--------------------------------
Proc 97: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 13 is on GPU 1
--------------------------------
Proc 13: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 68 is on GPU 0
--------------------------------
Proc 68: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 63 is on GPU 1
--------------------------------
Proc 63: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 4 is on GPU 0
--------------------------------
Proc 4: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 21 is on GPU 1
--------------------------------
Proc 21: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 40 is on GPU 0
--------------------------------
Proc 40: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 70 is on GPU 0
--------------------------------
Proc 70: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 82 is on GPU 0
--------------------------------
Proc 82: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 27 is on GPU 1
--------------------------------
Proc 27: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 85 is on GPU 1
--------------------------------
Proc 85: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 42 is on GPU 0
--------------------------------
Proc 42: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 23 is on GPU 1
--------------------------------
Proc 23: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 91 is on GPU 1
--------------------------------
Proc 91: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 0 is on GPU 0
Input file is: asdf/ta70000.h5
Output file is: asdf/ta70000.h5.res_all.h5
--------------------------------
Proc 0: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 55 is on GPU 1
--------------------------------
Proc 55: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 86 is on GPU 0
--------------------------------
Proc 86: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 56 is on GPU 0
--------------------------------
Proc 56: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 37 is on GPU 1
--------------------------------
Proc 37: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 93 is on GPU 1
--------------------------------
Proc 93: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 38 is on GPU 0
--------------------------------
Proc 38: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 81 is on GPU 1
--------------------------------
Proc 81: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 46 is on GPU 0
--------------------------------
Proc 46: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 64 is on GPU 0
--------------------------------
Proc 64: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 75 is on GPU 1
--------------------------------
Proc 75: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 15 is on GPU 1
--------------------------------
Proc 15: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 9 is on GPU 1
--------------------------------
Proc 9: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 45 is on GPU 1
--------------------------------
Proc 45: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 19 is on GPU 1
--------------------------------
Proc 19: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 59 is on GPU 1
--------------------------------
Proc 59: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 77 is on GPU 1
--------------------------------
Proc 77: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 29 is on GPU 1
--------------------------------
Proc 29: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 79 is on GPU 1
--------------------------------
Proc 79: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 89 is on GPU 1
--------------------------------
Proc 89: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 51 is on GPU 1
--------------------------------
Proc 51: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 61 is on GPU 1
--------------------------------
Proc 61: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 49 is on GPU 1
--------------------------------
Proc 49: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 3 is on GPU 1
--------------------------------
Proc 3: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 7 is on GPU 1
--------------------------------
Proc 7: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 73 is on GPU 1
--------------------------------
Proc 73: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 17 is on GPU 1
--------------------------------
Proc 17: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 11 is on GPU 1
--------------------------------
Proc 11: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 69 is on GPU 1
--------------------------------
Proc 69: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 53 is on GPU 1
--------------------------------
Proc 53: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 5 is on GPU 1
--------------------------------
Proc 5: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 41 is on GPU 1
--------------------------------
Proc 41: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 71 is on GPU 1
--------------------------------
Proc 71: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 83 is on GPU 1
--------------------------------
Proc 83: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 43 is on GPU 1
--------------------------------
Proc 43: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 1 is on GPU 1
--------------------------------
Proc 1: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 87 is on GPU 1
--------------------------------
Proc 87: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 57 is on GPU 1
--------------------------------
Proc 57: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 39 is on GPU 1
--------------------------------
Proc 39: *** testing PHDF5 dataset collective read...
--------------------------------
MPI process 47 is on GPU 1
--------------------------------
Proc 47: *** testing PHDF5 dataset collective read...
--------------------------------
[qb063:04642] 97 more processes have sent help message help-mpi-btl-openib.txt / ib port not selected
[qb063:04642] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[qb063:04642] 97 more processes have sent help message help-mpi-btl-openib.txt / error in device init
MPI process 65 is on GPU 1
--------------------------------
Proc 65: *** testing PHDF5 dataset collective read...
--------------------------------
Reading time is 1534.609
Elapsed time is 11249.389
Writing time is 126.008
===================================
PHDF5 tests finished with no errors
===================================
------------------------------------------------------
Running PBS epilogue script    - 20-May-2019 23:50:02
------------------------------------------------------
Checking node qb063 (MS)
Checking node qb146 ok
Checking node qb135 ok
Checking node qb134 ok
Checking node qb133 ok
Checking node qb132 ok
Checking node qb127 ok
Checking node qb124 ok
Checking node qb119 ok
Checking node qb118 ok
Checking node qb117 ok
Checking node qb116 ok
Checking node qb115 ok
Checking node qb114 ok
Checking node qb113 ok
Checking node qb112 ok
Checking node qb101 ok
Checking node qb098 ok
Checking node qb097 ok
Checking node qb096 ok
Checking node qb095 ok
Checking node qb094 ok
Checking node qb093 ok
Checking node qb092 ok
Checking node qb090 ok
Checking node qb089 ok
Checking node qb088 ok
Checking node qb087 ok
Checking node qb086 ok
Checking node qb085 ok
Checking node qb084 ok
Checking node qb083 ok
Checking node qb082 ok
Checking node qb081 ok
Checking node qb080 ok
Checking node qb078 ok
Checking node qb077 ok
Checking node qb076 ok
Checking node qb075 ok
Checking node qb074 ok
Checking node qb073 ok
Checking node qb072 ok
Checking node qb071 ok
Checking node qb069 ok
Checking node qb068 ok
Checking node qb067 ok
Checking node qb066 ok
Checking node qb065 ok
Checking node qb064 ok
Checking node qb063 ok
------------------------------------------------------
Concluding PBS epilogue script - 20-May-2019 23:51:39
------------------------------------------------------
Exit Status:     0
Job ID:          650125.qb3
Username:        fchen14
Group:           loniadmin
Job Name:        tn49.70k.pbs
Session Id:      3551
Resource Limits: ncpus=1,neednodes=49:ppn=20,nodes=49:ppn=20,walltime=72:00:00
Resources Used:  cput=06:36:07,mem=59608220kb,vmem=241353056kb,walltime=03:35:35
Queue Used:      checkpt
Account String:  loni_loniadmin1
Node:            qb063
Process id:      8258
Last status:     20-May-2019:23:05 PBS_job=650125.qb3 user=fchen14 allocation=loni_loniadmin1 queue=checkpt total_load=98.48 cpu_hours=171.98 wall_hours=1.96 unused_nodes=0 total_nodes=49 ppn=20 avg_load=2.00 avg_cpu=197% avg_mem=57349mb avg_vmem=231971mb top_proc=fchen14:pgi:qb127:113G:28G:1.8hr:102% toppm=fchen14:pgi.mpi.pacc.out:qb093:115874M:29090M gpupeak=190 node_processes=4 avg_avail_mem=2122mb min_avail_mem=670mb reverified_avg_load=2.00
------------------------------------------------------
