--------------------------------------
Running PBS prologue script
--------------------------------------
User and Job Data:
--------------------------------------
Job ID:    656263.qb3
Username:  fchen14
Group:     loniadmin
Date:      09-Jun-2019 18:28
Node:      qb370 (33035)
--------------------------------------
PBS has allocated the following nodes:

qb370
qb371
qb372
qb373

A total of 80 processors on 4 nodes allocated
---------------------------------------------
Check nodes and clean them of stray processes
---------------------------------------------
Checking node qb370 18:28:26 
Checking node qb371 18:28:27 
Checking node qb372 18:28:29 
Checking node qb373 18:28:31 
Done clearing all the allocated nodes
------------------------------------------------------
Concluding PBS prologue script - 09-Jun-2019 18:28:31
------------------------------------------------------
using 8 processes...
--------------------------------
Proc 2: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 3: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 6: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 4: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 7: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 5: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Input file is: hdf5t/sample_a-b_mix_2.h5
Output file is: hdf5t/sample_a-b_mix_2.h5.res_all.h5
--------------------------------
Proc 0: *** testing PHDF5 dataset collective read...
--------------------------------
Proc 1: *** testing PHDF5 dataset collective read...
--------------------------------
Reading time is 4.165
Elapsed time is 33.687
Writing time is 0.079
===================================
PHDF5 tests finished with no errors
===================================
omp run trd=5, 40 sec
Input file is: hdf5t/sample_a-b_mix_2.h5
Output file is: hdf5t/sample_a-b_mix_2.h5.res_all.h5
--------------------------------
Proc 1: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 0: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 6: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 7: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 2: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 4: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 3: *** testing PHDF5 dataset collective read...
--------------------------------
--------------------------------
Proc 5: *** testing PHDF5 dataset collective read...
--------------------------------
Reading time is 3.149
Elapsed time is 28.924
Writing time is 0.049
===================================
PHDF5 tests finished with no errors
===================================
omp run trd=10, 33 sec
Without hostfile option, hostnames must be specified on command line.
usage: mpirun_rsh [-v] [-sg group] [-rsh|-ssh] [-debug] -[tv] [-xterm] [-show] [-legacy] [-export] -np N(-hostfile hfile | h1 h2 ... hN) a.out args | -config configfile (-hostfile hfile | h1 h2 ... hN)]
Where:
	sg         => execute the processes as different group ID
	rsh        => to use rsh for connecting
	ssh        => to use ssh for connecting
	debug      => run each process under the control of gdb
	tv         => run each process under the control of totalview
	xterm      => run remote processes under xterm
	show       => show command for remote execution but don't run it
	legacy     => use old startup method (1 ssh/process)
	export     => automatically export environment to remote processes
	np         => specify the number of processes
	h1 h2...   => names of hosts where processes should run
or	hostfile   => name of file containing hosts, one per line
	a.out      => name of MPI binary
	args       => arguments for MPI binary
	config     => name of file containing the exe information: each line has the form -n numProc : exe args

omp run trd=5, 0 sec
Without hostfile option, hostnames must be specified on command line.
usage: mpirun_rsh [-v] [-sg group] [-rsh|-ssh] [-debug] -[tv] [-xterm] [-show] [-legacy] [-export] -np N(-hostfile hfile | h1 h2 ... hN) a.out args | -config configfile (-hostfile hfile | h1 h2 ... hN)]
Where:
	sg         => execute the processes as different group ID
	rsh        => to use rsh for connecting
	ssh        => to use ssh for connecting
	debug      => run each process under the control of gdb
	tv         => run each process under the control of totalview
	xterm      => run remote processes under xterm
	show       => show command for remote execution but don't run it
	legacy     => use old startup method (1 ssh/process)
	export     => automatically export environment to remote processes
	np         => specify the number of processes
	h1 h2...   => names of hosts where processes should run
or	hostfile   => name of file containing hosts, one per line
	a.out      => name of MPI binary
	args       => arguments for MPI binary
	config     => name of file containing the exe information: each line has the form -n numProc : exe args

omp run trd=10, 0 sec
------------------------------------------------------
Running PBS epilogue script    - 09-Jun-2019 18:29:45
------------------------------------------------------
Checking node qb370 (MS)
Checking node qb373 ok
Checking node qb372 ok
Checking node qb371 ok
Checking node qb370 ok
------------------------------------------------------
Concluding PBS epilogue script - 09-Jun-2019 18:29:52
------------------------------------------------------
Exit Status:     0
Job ID:          656263.qb3
Username:        fchen14
Group:           loniadmin
Job Name:        tn4_p8.mvp2.600.pbs
Session Id:      33034
Resource Limits: ncpus=1,neednodes=4:ppn=20,nodes=4:ppn=20,walltime=02:30:00
Resources Used:  cput=00:12:03,mem=10152kb,vmem=336004kb,walltime=00:01:13
Queue Used:      checkpt
Account String:  loni_loniadmin1
Node:            qb370
Process id:      33721
------------------------------------------------------
