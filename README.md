# Instruction

This README.md provides a brief introduction on how to use the distributed version to run 3D protein structure in parallel. The code uses two modes of hybrid - parallism, namely:
- Massage Passing Interface (MPI) + Open Multi-Processing (OpenMP)
  - This version runs on regular multi-core nodes (without GPU)
- Massage Passing Interface (MPI) + Open Accelerators (OpenACC)
  - This version runs on GPU nodes 

## How to compile the executable file

To compile the executable, type the below command:

```sh
$ make all
```
This will compile all two versions of the code, the MPI+OpenMP version can run on multi-core node (without GPU), 

 if only one version is needed, use the command:

```sh
$ make pomp
```

to compile the MPI + OpenMP version

Massage Passing Interface (MPI) + Open Multi-Processing (OpenMP)

## How to run jobs

### Memory usage consideration

The protein structure file is large and consumes a lot of memory when loaded to the program

### Usage of hdf5

Using hdf5 will greatly reduce the file size and also the speed when doing parallel input/output, therefore we use the hdf5 (.h5) as the input file for the protein structure and also for the output matrices (normal, generalises, wu and sarika).

### How to prepare input file

The input file is generated by running the below commands in the terminal.

```
$ python lf_csv2hdf5.py sample_hsp70_actin
```

The previous csv file will be converted to an hdf5 file named sample_hsp70_actin.h5 and will be used as the input file for the program.

### How to use output file

As mentioned earlier, the output of the program is also an hdf5 file with all the result information from each MPI process, however the information from each MPI process are not stored according to the original order, a python code will be used to gather information from the result hdf5 file and 







